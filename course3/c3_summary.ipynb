{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語のトークン化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 大文字小文字を区別しない\n",
    "* !などの記号は無視してくれる\n",
    "* カンマは認識しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index =  {'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'cat': 6}\n",
      "Sequences =  [[2, 3, 4, 5], [2, 3, 4, 6]]\n",
      "未出現の単語はindex1でprintされる [[4, 1, 1, 1, 1, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 入力する文をlistで宣言\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat'\n",
    "    ]\n",
    "\n",
    "# Tokenizerクラスの初期化\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "# 引数\n",
    "# num_words: 出現頻度の高い単語を何個まで使うか、これを超える単語は無視する\n",
    "# oov_token: 未知語を表す文字列 (out of vocabulary)\n",
    "\n",
    "# コーパスの各単語のインデックスを生成\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# コーパスの各単語のインデックス\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# トークン列のリストを生成する\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(\"Word Index = \", word_index)\n",
    "print(\"Sequences = \", sequences)\n",
    "print(\"未出現の単語はindex1でprintされる\", tokenizer.texts_to_sequences([\"my hobby is playing with my dog.\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student of waseda.\n"
     ]
    }
   ],
   "source": [
    "# 文中で出現頻度の高い単語の情報量は小さいので除去\n",
    "def remove_stopwords(sentence):\n",
    "    # 除去する単語のリスト\n",
    "    stopwords = [\"a\", \"and\", \"am\", \"i\", \"i'd\", \"you\"]\n",
    "    # 小文字に統一\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    words_list = sentence.split()  # 単語に分割\n",
    "    words_list = [w for w in words_list if w not in stopwords]\n",
    "    sentence = \" \".join(words_list)  # スペースを挟んで結合\n",
    "    return sentence\n",
    "\n",
    "print(remove_stopwords(\"I am a student of Waseda.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "* pad_sequencesで単語列の長さが一定になるように0埋めする\n",
    "* defaultだと0が先頭にくるが、padding='post'で末尾にすることができる\n",
    "* maxlenは最大の長さを指定できる（指定しないと最大の長さになる）\n",
    "* 単語列がmaxlenより長い場合は、defaultだと後ろからmaxlen分取得\n",
    "* 単語列の先頭からmaxlen分取得したい場合は、truncating='post'を指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences:\n",
      " [[2 3 4 5 0]\n",
      " [2 3 4 6 0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 配列の長さを一定にするためにPadding\n",
    "padded = pad_sequences(sequences, maxlen=5, padding='post', truncating='post')\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "\n",
    "print(\"Padded Sequences:\\n\", padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# GRADED FUNCTION: parse_data_from_file\n",
    "def parse_data_from_file(filename):\n",
    "    \"\"\"\n",
    "    Extracts sentences and labels from a CSV file\n",
    "\n",
    "    Args:\n",
    "        filename (string): path to the CSV file\n",
    "\n",
    "    Returns:\n",
    "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        ### START CODE HERE\n",
    "        reader = csv.reader(csvfile, delimiter=\",\")\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            sentences.append(remove_stopwords(row[1]))\n",
    "            labels.append(row[0])\n",
    "        ### END CODE HERE\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "embedding_dim = 16\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    # tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 学習グラフを描画して、validation_lossが小さくなっていることを確認する\n",
    "* 過学習が起きている場合、以下の対応が効果的かも\n",
    "  * vocab_sizeを減らす\n",
    "  * max_lengthを減らす"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "lstm_dim = 64\n",
    "dense_dim = 64\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim)),\n",
    "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* single layerのLSTM\n",
    "* multi layerのLSTM\n",
    "* convolutional layer\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('coursera-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93f3329071f9a5920dcc5fe9521be2d1f2b5252fdbb817e01c7799df210ae497"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
